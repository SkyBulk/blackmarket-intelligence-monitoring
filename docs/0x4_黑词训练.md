# 黑词训练

## 1. 探索阶段

该部分是正式设计黑词提取流程前的各种尝试。

### 1.1 导出训练用数据

使用NaviCat连接爬虫的MySQL数据库，导出message表的数据

<img src="https://image-host-toky.oss-cn-shanghai.aliyuncs.com/image-20201112162513197.png" alt="image-20201112162513197" style="zoom:50%;" />

图：NaviCat的表数据导出功能

<img src="https://image-host-toky.oss-cn-shanghai.aliyuncs.com/image-20201112162614913.png" alt="image-20201112162614913" style="zoom:50%;" />

图：导出的通讯消息文件




### 1.2 使用jieba分词

在探索阶段，直接使用jieba进行分词，并统计出词频，代码参考自[2]：

```python
# coding=utf-8
import jieba
import re
import time
from collections import Counter

#------------------------------------中文分词------------------------------------
cut_words = ""
all_words = ""
f = open('message_after_segment.txt', 'w')
for line in open('/Users/satan1a/Downloads/data-by-navicat/message.txt', encoding='utf-8'):
    line.strip('\n')
    seg_list = jieba.cut(line,cut_all=False)
    # print(" ".join(seg_list))
    cut_words = (" ".join(seg_list))
    f.write(cut_words)
    all_words += cut_words
else:
    f.close()

# 输出结果
all_words = all_words.split()
print(all_words)

# 词频统计
c = Counter()
for x in all_words:
    if len(x)>1 and x != '\r\n':
        c[x] += 1

# 输出词频最高的前10个词
print('\n词频统计结果：')
for (k,v) in c.most_common(10):
    print("%s:%d"%(k,v))

# 存储数据
name = time.strftime("%Y-%m-%d") + "-fc.csv"
fw = open(name, 'w', encoding='utf-8')
i = 1
for (k,v) in c.most_common(len(c)):
    # fw.write(str(i)+','+str(k)+','+str(v)+'\n')
    # ('背包问题', 10000),
    fw.write('(\''+str(k)+'\', ' + str(v) +'),')
    i = i + 1
else:
    print("Over write file!")
    fw.close()

```



分词后的文本效果：

<img src="https://image-host-toky.oss-cn-shanghai.aliyuncs.com/image-20201112163037264.png" alt="image-20201112163037264" style="zoom:50%;" />

图：进行jieba分词后的效果



然后我们进行词频统计：

![](https://image-host-toky.oss-cn-shanghai.aliyuncs.com/image-20201112163130529.png)

图：进行词频统计

### 1.3 绘制词云图

接下来我们使用统计出的词频绘制词云图，代码同样参考自[2]：

```python
# coding=utf-8
from pyecharts import options as opts
from pyecharts.charts import WordCloud
from pyecharts.globals import SymbolType

# 数据
words = [
    ('数据', 518030), ('飞机', 322917), ('代发', 297703), ('一手', 287843), ('联系', 266548), ('测试', 245080), ('劫持', 209523),
    ('合作', 208343), ('QQ', 201881), ('广告', 169091), ('长期', 158902), ('免费', 157373), ('棋牌', 154080), ('精准', 153463),
    ('通道', 148850), ('TG', 147242), ('微信', 144305), ('稳定', 139457), ('机房', 134278), ('欢迎', 125962), ('专业', 125023),
    ('卡发', 123602), ('国内', 121731), ('运营商', 116028), ('小时', 111486), ('https', 110846), ('me', 110667), ('发送', 108432),
    ('团队', 104830)
]

# 渲染图
def wordcloud_base() -> WordCloud:
    c = (
        WordCloud()
        .add("", words, word_size_range=[20, 100], shape='diamond')  # SymbolType.ROUND_RECT
        .set_global_opts(title_opts=opts.TitleOpts(title='Telegram黑灰产群组通讯信息 · 词云图'))
    )
    return c

# 生成图
wordcloud_base().render('词云图.html')
```



<img src="https://image-host-toky.oss-cn-shanghai.aliyuncs.com/image-20201112163353420.png" alt="image-20201112163353420" style="zoom:50%;" />

图：Telegram黑灰产群组通讯信息的词云图



<img src="https://image-host-toky.oss-cn-shanghai.aliyuncs.com/image-20201112163715730.png" alt="image-20201112163715730" style="zoom:50%;" />

图：增加统计元素后的词云图



## References

\[1] 基于文本挖掘的在线用户追加评论内容情报研究 ——以京东商城手机评论数据为例，张艳丰

\[2] [Pyhon疫情大数据分析] 三.新闻信息抓取及词云可视化、文本聚类和LDA主题模型文本挖掘, [Eastmount](https://me.csdn.net/Eastmount), https://blog.csdn.net/Eastmount/article/details/104698926